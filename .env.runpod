# RunPod GPU Environment Configuration

# Server Configuration
API_HOST=0.0.0.0
PORT=8000
WORKERS=4

# Database Configuration (use internal service)
DATABASE_URL=postgresql://inference:changeme@postgres:5432/inference_db
POSTGRES_PASSWORD=changeme

# Redis Configuration (use internal service)
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=changeme

# Authentication (GENERATE NEW SECRET!)
SECRET_KEY=CHANGE_THIS_RUN_openssl_rand_hex_32
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Storage Configuration
STORAGE_TYPE=local
STORAGE_PATH=/tmp/inference_storage
# For S3: STORAGE_TYPE=s3, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, S3_BUCKET

# Model Configuration
MODEL_CACHE_DIR=/tmp/model_cache
MAX_MODEL_CACHE_SIZE_GB=50

# Worker Configuration
CELERY_BROKER_URL=redis://:changeme@redis:6379/0
CELERY_RESULT_BACKEND=redis://:changeme@redis:6379/0
WORKER_CONCURRENCY=2

# GPU Configuration (ENABLED for RunPod)
ENABLE_GPU=true
DEFAULT_TIMEOUT=300
MAX_BATCH_SIZE=8

# CUDA Configuration
CUDA_VISIBLE_DEVICES=0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
