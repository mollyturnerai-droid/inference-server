version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: inference
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_DB: inference_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U inference"]
      interval: 10s
      timeout: 5s
      retries: 5

  web:
    build:
      context: ./inference-web-ui
      dockerfile: Dockerfile
    ports:
      - "${WEB_PORT:-3000}:3000"

  redis:
    image: redis:7-alpine
    command: redis-server --requirepass ${REDIS_PASSWORD:-changeme}
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  api:
    image: ${DOCKERHUB_USERNAME}/inference-server:${IMAGE_TAG:-gpu}
    ports:
      - "${PORT:-8000}:8000"
    environment:
      - DATABASE_URL=postgresql://inference:${POSTGRES_PASSWORD:-changeme}@postgres:5432/inference_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-changeme}
      - SECRET_KEY=${SECRET_KEY}
      - API_KEY=${API_KEY}
      - ENABLE_GPU=true
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD:-changeme}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD:-changeme}@redis:6379/0
      - STORAGE_TYPE=${STORAGE_TYPE:-local}
      - WORKER_CONCURRENCY=${WORKER_CONCURRENCY:-2}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - model_cache:/tmp/model_cache
      - storage:/tmp/inference_storage

  worker:
    image: ${DOCKERHUB_USERNAME}/inference-server:${IMAGE_TAG:-gpu}
    command: ["sh", "-c", "celery -A app.workers.celery_app worker --loglevel=info --concurrency=${WORKER_CONCURRENCY:-2} --pool=${CELERY_WORKER_POOL:-solo}"]
    environment:
      - DATABASE_URL=postgresql://inference:${POSTGRES_PASSWORD:-changeme}@postgres:5432/inference_db
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-changeme}
      - SECRET_KEY=${SECRET_KEY}
      - ENABLE_GPU=true
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD:-changeme}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD:-changeme}@redis:6379/0
      - WORKER_CONCURRENCY=${WORKER_CONCURRENCY:-2}
      - CELERY_WORKER_POOL=${CELERY_WORKER_POOL:-solo}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - model_cache:/tmp/model_cache
      - storage:/tmp/inference_storage

  mcp_gateway:
    image: ${DOCKERHUB_USERNAME}/inference-server:${IMAGE_TAG:-gpu}
    command: ["uvicorn", "mcp_gateway.main:app", "--host", "0.0.0.0", "--port", "8001"]
    ports:
      - "${MCP_PORT:-8001}:8001"
    environment:
      - INFERENCE_BASE_URL=http://api:8000
      - CATALOG_ADMIN_TOKEN=${CATALOG_ADMIN_TOKEN}
      - API_KEY=${API_KEY}
      - MCP_GATEWAY_TIMEOUT_SECONDS=${MCP_GATEWAY_TIMEOUT_SECONDS:-120}
    depends_on:
      api:
        condition: service_started

volumes:
  postgres_data:
  redis_data:
  model_cache:
  storage:
